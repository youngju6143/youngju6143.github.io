---
layout: post
title:  "[혼공머신 ch6]"
date:   2024-08-19 13:08:38 +0900
categories: ML Session
---

## Chapter 6 : 비지도 학습

### 6-1 군집 알고리즘

> 타깃을 모르는 비지도 학습

<font color='#1E90FF'>비지도 학습(unsupervised learning)</font> ? 타깃이 없을 때 사용하는 머신러닝 알고리즘
- 사이킷런이 결정 트리 알고리즘 제공 (DecisionTreeClassifier)
    - plot_tree() : 결정 트리를 이해하기 쉬운 트리 그림으로 출력해주는 함수
- <font color='#FF69B4'>표준화 전처리가 필요 없음!!!!</font>

> 과일 사진 데이터 준비하기

**↓ 과일 사진 데이터 준비하기 코드**
```
!wget https://bit.ly/fruits_300 -O fruits_300.npy
```
***!*** : 리눅스 셸 명령으로 이해하게 해주는 문자<br/>
***wget*** : 원격 주소에서 데이터를 다운로드하여 저장<br/>
***-O*** : 저장할 파일 이름을 지정 (fruits_300.npy)<br/>
```
import numpy as np
import matplotlib.pyplot as plt

fruits = np.load('fruits_300.npy')
print(fruits.shape) # (300, 100, 100)
```
첫 번째 차원(300) : 샘플의 개수 <br/>
두 번째 차원(100) : 이미지의 높이 <br/>
세 번째 차원(100) : 이미지의 너비 <br/>
=> 이미지의 크기 : 100 X 100 <br/>
```
# 첫 번째 이미지의 첫 번째 행 출력
print(fruits[0, 0, :])

'''
[  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   1
   2   2   2   2   2   2   1   1   1   1   1   1   1   1   2   3   2   1
   2   1   1   1   1   2   1   3   2   1   3   1   4   1   2   5   5   5
  19 148 192 117  28   1   1   2   1   4   1   1   3   1   1   1   1   1
   2   2   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1
   1   1   1   1   1   1   1   1   1   1]
'''
```
픽셀 100개에 들어있는 값을 출력함 <br/>
**0에 가까울수록 검게** 나타나고 값이 **높을수록 밝게** 표시됨
```
plt.imshow(fruits[0], cmap='gray')
plt.show()
```
***imshow*** : 넘파이 배열로 저장된 이미지를 그림
- ***cmap='gray'*** : 흑백으로 저장
- ***cmap='gray_r'*** : 반전 흑백으로 확인

<img width=200 src="https://github.com/user-attachments/assets/41838b2f-ba8a-4b03-a827-933eb07f718f"/>

실제 사진은 바탕이 흰색, 사과가 더 짙은색이나, 출력하면 반전됨<br/>
이유는 ? 사진으로 찍은 이미지를 <font color='#FF69B4'>넘파이 배열로 변환할 때 반전</font>시켰기 때문!

<img width=200 src="https://github.com/user-attachments/assets/e86317ea-7b4b-4ff4-ba2c-37f9678432ff"/>

굳이 이렇게 바꾼 이유는 ?<br/>
컴퓨터는 수치로 해석, 0인 값에 초점을 맞추게 되면 아무리 곱하거나 더해도 0이므로<br/>
밝은 색인 255에 초점을 맞춰야 함 -> 우리가 확인하고 싶은 건 **사과** <br/>
<font color='#FF69B4'>=> 따라서 사과를 밝은 색으로 해야 하기 위해 색을 반전시킴</font> 

**↓ cmap='gray_r'을 통해 원래의 색상으로 확인 가능**
```
fig, axs = plt.subplots(1, 2)
axs[0].imshow(fruits[100], cmap='gray_r')
axs[1].imshow(fruits[200], cmap='gray_r')
plt.show()
```
***subplots()*** : 여러 개의 그래프를 배열처럼 쌓을 수 있도록 함
<img width=300 src="https://github.com/user-attachments/assets/bfb1b763-ab9b-4df1-8b41-872c6e469777"/>

> 픽셀값 분석하기

**↓ 배열로 계산하기 위해 100 X 100 이미지를, 길이가 10000인 1차원 배열로 만듦**
```
apple = fruits[0:100].reshape(-1, 100*100)
pineapple = fruits[100:200].reshape(-1, 100*100)
banana = fruits[200:300].reshape(-1, 100*100)

print(apple.shape) # (100, 10000)
```
샘플마다 픽셀의 평균값을 계산해야 함
- mean() 메서드 사용

**↓ 평균값 계산하는 코드**
```
# 샘플의 평균값 계산

print(apple.mean(axis=1)) // 각 샘플의 픽셀 평균값 계산

# 픽셀별 평균값 비교 (not 샘플의 평균값)

fig, axs = plt.subplots(1, 3, figsize=(20, 5))
axs[0].bar(range(10000), np.mean(apple, axis=0))
axs[1].bar(range(10000), np.mean(pineapple, axis=0))
axs[2].bar(range(10000), np.mean(banana, axis=0))
plt.show()
```
<img width=300 src="https://github.com/user-attachments/assets/50964a10-cc95-4190-847e-f367bc1ee2f5"/> <br/>

높은 부분 - 사과 : 아래쪽 / 파인애플 : 고르게 높음 / 바나나 : 중앙쪽
```
apple_mean = np.mean(apple, axis=0).reshape(100, 100)
pineapple_mean = np.mean(pineapple, axis=0).reshape(100, 100)
banana_mean = np.mean(banana, axis=0).reshape(100, 100)

fig, axs = plt.subplots(1, 3, figsize=(20, 5))
axs[0].imshow(apple_mean, cmap='gray_r')
axs[1].imshow(pineapple_mean, cmap='gray_r')
axs[2].imshow(banana_mean, cmap='gray_r')
plt.show()
```

<img width=300 src="https://github.com/user-attachments/assets/20660ead-583b-45a6-aaf2-d1818f82724c"/>

> 평균값과 가까운 사진 고르기

평균값과 가까운 사진 고르는 방법 : <font color='#FF69B4'>절댓값 오차</font>를 사용하여 오차가 가장 작은 이미지 선정!!
 - 절댓값 구하는 함수 : ***abs()***

**↓ 절댓값 오차 계산하는 코드**
```
# 절댓값 계산

abs_diff = np.abs(fruits - apple_mean)
abs_mean = np.mean(abs_diff, axis=(1, 2)) # 각 샘플의 오차 평균
print(abs_mean.shape) # (300,)
```
```
# apple_mean과 오차가 가장 작은 샘플 100개 선정

apple_index = np.argsort(abs_mean)[:100]
fig, axs = plt.subplots(10, 10, figsize=(10, 10))

for i in range(10): 
  for j in range(10):
    axs[i, j].imshow(fruits[apple_index[i*10 + j]], cmap='gray_r')
    axs[i, j].axis('off')

plt.show()
```
***np.argsort()*** : 작은 것에서 큰 순서대로 나열한 abs_mean 배열의 인덱스를 반환하는 함수 <br/>
***axis('off')*** : 축 제거

<img width=200 src="https://github.com/user-attachments/assets/c8fb4d5c-a338-45e8-b892-fb2e4a323c5d"/>

<font color='#1E90FF'>군집(clustering)</font> ? 비슷한 샘플끼리 그룹으로 모으는 작업
- 위 코드에서와 같이 비슷한 샘플 100개를 모아 출력
- 대표적인 비지도 학습 중 하나

<font color='#1E90FF'>클러스터(cluster)</font> ? 군집 알고리즘에서 만든 그룹

---
### 6-2 평균
> k-평균 알고리즘 소개

실제로는 비지도 학습에서 타깃값을 모르기 때문에 샘플의 평균값을 미리 구할 수 없음

그렇다면 타깃값을 모르는데 어떻게 세 과일의 평균값을 찾을 수 있었을까??

=> <font color='#1E90FF'>k-평균(k-means) 군집 알고리즘</font>이 평균값을 자동으로 찾아줌
- 이 평균값을 <font color='#1E90FF'>클러스터 중심(cluster center)</font> OR <font color='#1E90FF'>센트로이드(centroid)</font> 라고 부름

 

**k-평균(k-means) 알고리즘 작동 방식**
1. 무작위로 k개의 클러스터 중심을 정한다.
2. 각 샘플에서 가장 가까운 클러스터 중심을 찾아 해당 클러스터의 샘플로 지정한다.
3. 클러스터에 속한 샘플의 평균값으로 클러스터 중심을 변경한다.
4. 클러스터 중심에 변화가 없을 때까지 2번으로 돌아가 반복한다.

<img width=400 src="https://github.com/user-attachments/assets/7cf6b955-13cb-4ec6-821e-98c08d64e29f"/>

1. 3개의 클러스터 중심을 랜덤하게 지정한다 <br/>
<font color=grey>
-> 클러스터 중심에서 가장 가까운 샘플들을 하나의 클러스터로 묶는다<br/>
(클러스터에서 순서나 번호는 의미 X)<br/>
-> 클러스터의 중심을 다시 계산하여 이동<br/>
    - 1번에서 2사과 1파인애플의 경우, 클러스터의 중심이 더 많은 사과쪽으로 이동
</font>
2. 다시 계산한 클러스터 중심에서 가장 가까운 샘플들을 하나의 클러스터로 묶는다.<br/>
<font color=grey>
->  클러스터의 중심을 다시 계산하여 이동
</font>
3. 다시 계산한 클러스터 중심에서 가장 가까운 샘플들을 하나의 클러스터로 묶는다.<br/>
    - 2번 클러스터와 동일 <br/>
<font color=grey>
-> 만들어진 클러스터에 변동이 없으므로 k-평균 알고리즘을 종료!!
</font>

> KMeans 클래스

사이킷런에 ***KMeans 클래스*** 구현되어 있음!!
- ***n_cluster*** : 클러스터 개수 지정

**↓  KMeans 클래스 사용하는 코드**
```
import numpy as np
fruits = np.load('fruits_300.npy')
fruits_2d = fruits.reshape(-1, 100*100)

from sklearn.cluster import KMeans
km = KMeans(n_clusters=3, random_state=42)
km.fit(fruits_2d)

print(km.labels_)
```
비지도 학습이므로 fit() 에서 타깃 데이터를 사용하지 않음!!!

군집된 결과는 KMeans 클래스 객체의 ***labels_*** 속성에 저장됨

```
# 레이블로 모은 샘플의 개수 확인
print(np.unique(km.labels_, return_counts=True))

# (array([0, 1, 2], dtype=int32), array([ 91,  98, 111]))
```
첫 번째 클러스터(0) : 91 <br/>
두 번째 클러스터(1) : 98 <br/>
세 번째 클러스터(2) : 111 <br/>

```
# 각 클러스터가 어떤 이미지를 나타냈는지 그림으로 출력하는 함수 출력

import matplotlib.pyplot as plt
def draw_fruits(arr, ratio=1):
  n = len(arr) # 샘플 개수
  rows = int(np.ceil(n/10)) # 한 줄에 10개씩 이미지 그림
  cols = n if rows < 2 else 10 # 행의 개수==1 ? 열의개수==샘플개수 : 열의개수==10

  fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False)

  for i in range(rows):
    for j in range(cols):
      if i*10 + j < n:
        axs[i, j].imshow(arr[i*10 + j], cmap='gray_r')
      axs[i, j].axis('off')

  plt.show()
```
```
# 불리언 인덱싱

draw_fruits(fruits[km.labels_==0])
# km.labels_ 배열에서 값이 0인 위치는 True, 그 외는 False
```

> 클러스터 중심

***KMeans 클래스***가 최종적으로 찾은 클러스터 중심은 ***cluster_centers*** 속성에 저장되어 있음
- ***transform()*** : 훈련 데이터 샘플에서 클러스터 중심까지 거리로 변환해주는 메서드

**↓ 코드**
```
# 2차원 배열로 바꿈
draw_fruits(km.cluster_centers_.reshape(-1, 100, 100), ratio=3)
```
```
# 인덱스가 100인 샘플에 transform() 메서드 적용

print(km.transform(fruits_2d[100:101]))
#[[5267.70439881 8837.37750892 3393.8136117 ]]

print(km.predict(fruits_2d[100:101]))
# [2]
```
index=2가 3393으로 제일 가까움 -> 파인애플로 예측

> 최적의 k 찾기

k-평균 알고리즘의 단점 : 클러스터 개수를 사전에 지정해야 함 <br/>
그렇다면 어떻게 최적의 클러스터 개수를 찾을 수 있을까?<br/>
=> <font color='#1E90FF'>엘보우</font> 사용

<font color='#1E90FF'>엘보우(elbow)</font> ? 클러스터 개수를 늘려가면서 <font color='#1E90FF'>이너셔</font>의 변화를 관찰하여 최적의 클러스터 개수를 찾는 방법
- <font color='#1E90FF'>이너셔(inertia)</font> ? 클러스터의 샘플이 얼마나 가깝게 있는지를 나타내는 값
    - 클러스터 중심과 클러스터에 속한 샘플 사이의 거리의 제곱합으로 나타냄
    - 일반적으로 클러스터 개수 ↑ -> 클러스터 개개의 크기 ↓ -> 이너셔 ↓
    - KMeans 클래스에서 자동으로 이너셔를 계산 -> inertia_ 속성으로 제공

<img width=300 src="https://github.com/user-attachments/assets/f9b834f1-71ca-430b-b193-6b452544a213"/>

이너셔의 그래프에서 <font color='#FF69B4'>꺾이는 지점이 최적의 클러스터 개수 !!</font>

**↓  이너셔를 그래프로 나타내는 코드**
```
inertia = []
for k in range(2, 7): # 클러스터 개수를 2~6까지 바꿔가며 훈련
  km = KMeans(n_clusters=k, random_state=42)
  km.fit(fruits_2d)
  inertia.append(km.inertia_) 

plt.plot(range(2, 7), inertia)
plt.show()
```

<img width=300 src="https://github.com/user-attachments/assets/66bdd42e-72f6-4cc6-b2ea-cdd5ad2dff24"/>

k=3인 곳에서 꺾임 -> 3이 최적의 클러스터 개수!!

---

### 6-3 주성분 분석
> 차원과 차원 축소

<font color='#1E90FF'>차원(dimension)</font> ? 특성의 또 다른 이름 <br/>
<font color=grey>- ex) 10,000개의 특성이 있다 == 10,000개의 차원이 있다</font>

<font color='#1E90FF'>차원 축소(dimensionally reduction)</font> ? 데이터를 가장 잘 나타내는 일부 특성을 선택하여 데이터 크기를 줄이고 지도 학습 모델의 성능을 향상시킬 수 있는 방법
-  대표적인 차원 축소 알고리즘 : <font color='#1E90FF'>주성분 분석(principal component analysis)</font> (a.k.a <font color='#1E90FF'>PCA</font>)

> 주성분 분석 소개

<font color='#1E90FF'>주성분 분석(principal component analysis)</font> ? 데이터에 있는 분산이 큰 방향을 찾는 방법
- 분산 ? 데이터가 멀리 퍼져있는 정도

<img width=300 src="https://github.com/user-attachments/assets/05ffc2cc-67f7-41a2-8939-8429210f2414"/>

위 그래프의 경우, 대각선 방향이 분산이 크다는 것을 알 수 있음
- 화살표의 위치는 상관 X

<img width=300 src="https://github.com/user-attachments/assets/b258c7d9-b1be-4777-bedb-5cac2faca8ab"/>

이 직선을 원점으로 옮긴 후. 두 원소로 이루어진 벡터로 쓸 수 있음<br/>
=> <font color='#1E90FF'>주성분(principal component)</font> 라고 부름

주성분 벡터 : 원본 데이터에 있는 어떤 방향<br/>
=> 주성분 벡터의 원소 개수 == 원본 데이터셋에 있는 특성 개수

원본 데이터 : 주성분을 사용하여 차원 축소 가능 -> 투영

<img width=300 src="https://github.com/user-attachments/assets/f860aa67-15da-44fb-9e16-e1a9cbb0a8f4"/>

> PCA 클래스

**↓  PCA 클래스를 사용하여 차원 축소하는 코드**
```
from sklearn.decomposition import PCA
pca = PCA(n_components=50)
pca.fit(fruits_2d)

print(pca.components_.shape) # (50, 10000)
```
***n_components*** : 주성분의 개수

PCA 클래스가 찾은 주성분은 ***components_*** 속성에 저장
- 첫 번째 차원 : 주성분의 개수가 50이므로 50
- 두 번째 차원 : 원본 데이터의 특성 개수와 같은 10000

```
# 주성분을 이미지처럼 출력
draw_fruits(pca.components_.reshape(-1, 100, 100))
```

<img width=300 src="https://github.com/user-attachments/assets/a4e2addc-e206-4942-9642-f8b3859d6a9f"/>

원본 데이터에서 가장 분산이 큰 방향을 순서대로 나타냄
```
# 주성분을 찾았으니, 차원 줄이기
print(fruits_2d.shape) # (300, 10000)
fruits_pca = pca.transform(fruits_2d)

print(fruits_pca.shape) # (300, 50)
```

> 원본 데이터 재구성

***inverse_transform()*** : 원본 데이터를 상당 부분 재구성하는 메서드

**↓  원본 데이터 재구성하는 코드**
```
fruits_inverse = pca.inverse_transform(fruits_pca)
print(fruits_inverse.shape) # (300, 10000)
```
```
# 복원된 데이터를 100개씩 나누어 출력
fruits_reconstruct = fruits_inverse.reshape(-1, 100, 100)
for start in [0, 100, 200]:
  draw_fruits(fruits_reconstruct[start:start+100])
  print("\n")
```
<img width=400 src="https://github.com/user-attachments/assets/eef62027-40f1-4c74-8e2a-9858cbfa7925"/>

> 설명된 분산>

<font color='#1E90FF'>설명된 분산(explained variance)</font> ? 주성분이 원본 데이터의 분산을 얼마나 잘 나타내는지 기록한 값
- PCA 클래스의 ***explained_variance_ratio_*** 에 설명된 분산 비율이 기록되어 있음
- 첫 번째 주성분의 설명된 분산이 가장 큼

**↓  설명된 분산, 총 분산 비율 구하는 코드**
```
# 총 분산 비율

print(np.sum(pca.explained_variance_ratio_))
# 0.9215333578580416
```
=> 복원했을 때의 원본 이미지의 품질이 높은 이유임!!

<br/>

> 다른 알고리즘과 함께 사용하기


**↓  로지스틱 회귀 알고리즘과 같이 사용하는 코드**
```
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()

target = np.array([0]*100 + [1]*100 + [2]*100)
```
```
# cross_validate 사용해서 교차 검증 실행
from sklearn.model_selection import cross_validate
scores = cross_validate(lr, fruits_2d, target)
print(np.mean(scores['test_score']))
print(np.mean(scores['fit_time']))

# 0.9966666666666667
# 1.6808361053466796
```
```
# PCA로 축소한 fruits_pca를 사용했을 떄와 비교

scores = cross_validate(lr, fruits_pca, target)
print(np.mean(scores['test_score']))
print(np.mean(scores['fit_time']))

# 1.0
# 0.07098498344421386
```
=> 훈련 시간이 1.6 -> 0.07로 대폭 감소됨!!

<font color='#FF69B4'>=> PCA로 훈련 데이터의 차원을 축소하면 저장 공간뿐만 아니라 모델의 훈련 속도도 높일 수 있다!!</font>

```
# 설명된 분산의 50%에 달하는 주성분 찾기

pca = PCA(n_components=0.5)
pca.fit(fruits_2d)

print(pca.n_components_) # 2
```
=> 단 2개의 특성만으로 원본 데이터에 있는 분산의 50% 표현 가능!!
```
# 원본 데이터로 변환하기

fruits_pca = pca.transform(fruits_2d)
print(fruits_pca.shape) # (300, 2)
```
주성분이 2개이므로 변환된 데이터의 크기는 (300, 2)
```
# 교차 검증

scores = cross_validate(lr, fruits_pca, target)
print(np.mean(scores['test_score']))
print(np.mean(scores['fit_time']))

# 0.9933333333333334
# 0.030288887023925782
```

**↓  k-평균 알고리즘으로 클러스터 찾는 코드**
```
# 차원 축소된 데이터를 사용해 k-평균 알고리즘으로 클러스터 찾기

from sklearn.cluster import KMeans
km = KMeans(n_clusters=3, random_state=42)
km.fit(fruits_pca)
print(np.unique(km.labels_, return_counts=True))

# (array([0, 1, 2], dtype=int32), array([110,  99,  91]))
```
<img width=400 src="https://github.com/user-attachments/assets/e7262b70-188b-470d-a55f-f232b66ee451"/>

```
# KMeans가 찾은 레이블을 사용하여 과일 이미지 출력

for label in range(0, 3):
  draw_fruits(fruits[km.labels_ == label])
  print("\n")
```
<img width=200 src="https://github.com/user-attachments/assets/f2898426-336e-443c-bac8-2e5c6902370c"/>

```
# km.labels_을 사용해 클러스터별로 나누고 산점도 그리기

for label in range(0, 3):
  data = fruits_pca[km.labels_ == label]
  plt.scatter(data[:, 0], data[:, 1])
plt.legend(['apple', 'banana', 'pineapple'])
plt.show()
```
